# SPDX-FileCopyrightText: 2025 Knitli Inc.
# SPDX-FileContributor: Adam Poulemanos <adam@knit.li>
#
# SPDX-License-Identifier: MIT OR Apache-2.0

name: Weekly Comprehensive Tests

permissions:
  contents: read
  issues: write

on:
  schedule:
    # Run every Sunday at 3 AM UTC
    - cron: "0 3 * * 0"
  workflow_dispatch:
    inputs:
      include-platforms:
        description: Test on all platforms (linux, windows, macos)
        required: false
        type: boolean
        default: true
      include-benchmarks:
        description: Run performance benchmarks
        required: false
        type: boolean
        default: true

concurrency:
  group: weekly-tests-${{ github.ref }}
  cancel-in-progress: true

jobs:
  # Comprehensive test matrix across all platforms
  weekly-comprehensive-linux:
    name: Weekly Tests - Linux
    uses: ./.github/workflows/_reusable-test.yml
    secrets:
      CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}
      CODEWEAVER_VECTOR_STORE_URL: ${{ secrets.CODEWEAVER_VECTOR_STORE_URL }}
      QDRANT__SERVICE__API_KEY: ${{ secrets.QDRANT__SERVICE__API_KEY }}
      VOYAGE_API_KEY: ${{ secrets.VOYAGE_API_KEY }}
    with:
      python-versions: '["3.12", "3.13", "3.14"]'
      # Run everything except docker, qdrant, skip_ci, and flaky tests
      test-markers: "not docker and not qdrant and not skip_ci and not flaky and not windows_only and not macos_only"
      upload-coverage: true
      run-quality-checks: true

  weekly-comprehensive-windows:
    name: Weekly Tests - Windows
    uses: ./.github/workflows/_reusable-test.yml
    if: ${{ github.event_name == 'schedule' || inputs.include-platforms }}
    secrets:
      CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}
      CODEWEAVER_VECTOR_STORE_URL: ${{ secrets.CODEWEAVER_VECTOR_STORE_URL }}
      QDRANT__SERVICE__API_KEY: ${{ secrets.QDRANT__SERVICE__API_KEY }}
      VOYAGE_API_KEY: ${{ secrets.VOYAGE_API_KEY }}
    with:
      python-versions: '["3.12", "3.13", "3.14"]'
      # Run everything except docker, qdrant, skip_ci, and flaky tests
      test-markers: "not docker and not qdrant and not skip_ci and not flaky and not linux_only and not macos_only"
      upload-coverage: true
      run-quality-checks: true
      runner-version: windows-latest

  weekly-comprehensive-macos:
    name: Weekly Tests - macOS
    uses: ./.github/workflows/_reusable-test.yml
    if: ${{ github.event_name == 'schedule' || inputs.include-platforms }}
    secrets:
      CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}
      CODEWEAVER_VECTOR_STORE_URL: ${{ secrets.CODEWEAVER_VECTOR_STORE_URL }}
      QDRANT__SERVICE__API_KEY: ${{ secrets.QDRANT__SERVICE__API_KEY }}
      VOYAGE_API_KEY: ${{ secrets.VOYAGE_API_KEY }}
    with:
      python-versions: '["3.12", "3.13", "3.14"]'
      # Run everything except docker, qdrant, skip_ci, and flaky tests
      test-markers: "not docker and not qdrant and not skip_ci and not flaky and not linux_only and not windows_only"
      upload-coverage: true
      run-quality-checks: true
      runner-version: macos-latest

  weekly-benchmarks:
    name: Weekly Performance Benchmarks
    uses: ./.github/workflows/_reusable-test.yml
    if: ${{ github.event_name == 'schedule' || inputs.include-benchmarks }}
    secrets:
      CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}
      CODEWEAVER_VECTOR_STORE_URL: ${{ secrets.CODEWEAVER_VECTOR_STORE_URL }}
      QDRANT__SERVICE__API_KEY: ${{ secrets.QDRANT__SERVICE__API_KEY }}
      VOYAGE_API_KEY: ${{ secrets.VOYAGE_API_KEY }}
    with:
      benchmark-tests: true
      python-versions: '["3.12"]'
      run-quality-checks: true
      upload-benchmark-results: true
  weekly-coverage-report:
    name: Weekly Coverage Report
    runs-on: ubuntu-latest
    needs: weekly-comprehensive-linux
    env:
      MISE_EXPERIMENTAL: 1
      MISE_YES: 1
      PROFILE: "dev"
      CODEWEAVER_PROJECT_PATH: ${{ github.workspace }}
      CODEWEAVER_TEST_MODE: "true"
      MISE_PYTHON_VERSION: "3.12"
      UV_PYTHON: "3.12"
    steps:
      - name: Checkout code
        uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8
        with:
          fetch-depth: 3

      - name: Setup Python environment with Mise
        uses: ./.github/actions/setup-mise-env
        with:
          python-version: "3.12"
          github-token: ${{ secrets.GITHUB_TOKEN }}
          profile: dev
          skip-checkout: true
        id: setup-mise

      - name: Cache embedding models
        uses: actions/cache@0057852bfaa89a56745cba8c7296529d2fc39830
        with:
          path: |
            ~/.cache/huggingface/
            ~/.cache/sentence_transformers/
          key: embedding-models-v1-${{ runner.os }}-${{ hashFiles('pyproject.toml', 'uv.lock', 'codeweaver.test.toml') }}
          restore-keys: |
            embedding-models-v1-${{ runner.os }}-

      - name: Run full coverage analysis
        run: mise run test-cov -m "not docker and not qdrant and not skip_ci and not flaky"

      - name: Generate coverage report
        run: |
          # Generate detailed HTML coverage report
          uv run coverage html
          uv run coverage report > coverage-report.txt

      - name: Upload coverage report
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02
        with:
          name: coverage-report
          path: |
            htmlcov/
            coverage-report.txt
            coverage.xml
          retention-days: 30

      - name: Upload to Codecov
        uses: codecov/codecov-action@5a1091511ad55cbe89839c7260b706298ca349f7
        with:
          files: ./coverage.xml
          fail_ci_if_error: false
          token: ${{ secrets.CODECOV_TOKEN }}
          flags: weekly

  weekly-summary:
    name: Weekly Test Summary
    runs-on: ubuntu-latest
    needs:
      - weekly-comprehensive-linux
      - weekly-comprehensive-windows
      - weekly-comprehensive-macos
      - weekly-benchmarks
      - weekly-coverage-report
    if: always()
    permissions:
      contents: read
      issues: write
    steps:
      - name: Download benchmark results
        if: needs.weekly-benchmarks.result != 'skipped'
        uses: actions/download-artifact@fa0a91b85d4f404e444e00e005971372dc801d16
        continue-on-error: true
        with:
          name: benchmark-results
          path: ./benchmarks/

      - name: Download benchmark summary
        if: needs.weekly-benchmarks.result != 'skipped'
        uses: actions/download-artifact@fa0a91b85d4f404e444e00e005971372dc801d16
        continue-on-error: true
        with:
          name: benchmark-summary
          path: ./benchmarks/

      - name: Check test results
        id: check-results
        env:
          linux_result: ${{ needs.weekly-comprehensive-linux.result }}
          windows_result: ${{ needs.weekly-comprehensive-windows.result }}
          macos_result: ${{ needs.weekly-comprehensive-macos.result }}
          benchmark_result: ${{ needs.weekly-benchmarks.result }}
          coverage_result: ${{ needs.weekly-coverage-report.result }}
        run: |
          echo "Linux Tests: ${linux_result}"
          echo "Windows Tests: ${windows_result}"
          echo "macOS Tests: ${macos_result}"
          echo "Benchmarks: ${benchmark_result}"
          echo "Coverage Report: ${coverage_result}"

          # Calculate overall status
          critical_failure=false
          platform_warnings=false

          if [ "$linux_result" == "failure" ]; then
            critical_failure=true
          fi

          if [ "$windows_result" == "failure" ] || [ "$macos_result" == "failure" ]; then
            platform_warnings=true
          fi

          if [ "$critical_failure" == "true" ]; then
            echo "status=failure" >> "$GITHUB_OUTPUT"
            echo "Overall Status: FAILURE âŒ"
          elif [ "$platform_warnings" == "true" ]; then
            echo "status=warning" >> "$GITHUB_OUTPUT"
            echo "Overall Status: WARNING âš ï¸ (Platform-specific failures)"
          else
            echo "status=success" >> "$GITHUB_OUTPUT"
            echo "Overall Status: SUCCESS âœ…"
          fi

      - name: Generate comprehensive summary
        run: |
          {
            echo "# Weekly Comprehensive Test Results ðŸ“Š"
            echo ""
            echo "**Week Ending**: $(date -u +"%Y-%m-%d")"
            echo "**Test Date**: $(date -u +"%Y-%m-%d %H:%M UTC")"
            echo "**Commit**: ${{ github.sha }}"
            echo ""
          } >> "${GITHUB_STEP_SUMMARY}"

          {
            echo "## Platform Test Results"
            echo ""
          } >> "${GITHUB_STEP_SUMMARY}"

          linux="${{ needs.weekly-comprehensive-linux.result }}"
          windows="${{ needs.weekly-comprehensive-windows.result }}"
          macos="${{ needs.weekly-comprehensive-macos.result }}"

          # Linux Tests
          if [ "${linux}" == "success" ]; then
            echo "- âœ… **Linux**: PASSED (Python 3.12, 3.13, 3.14)" >> "${GITHUB_STEP_SUMMARY}"
          elif [ "${linux}" == "failure" ]; then
            echo "- âŒ **Linux**: FAILED" >> "${GITHUB_STEP_SUMMARY}"
          else
            echo "- âš ï¸ **Linux**: ${linux}" >> "${GITHUB_STEP_SUMMARY}"
          fi

          # Windows Tests
          if [ "${windows}" == "success" ]; then
            echo "- âœ… **Windows**: PASSED (Python 3.12, 3.13)" >> "${GITHUB_STEP_SUMMARY}"
          elif [ "${windows}" == "failure" ]; then
            echo "- âš ï¸ **Windows**: FAILED (Non-blocking)" >> "${GITHUB_STEP_SUMMARY}"
          elif [ "${windows}" == "skipped" ]; then
            echo "- â­ï¸ **Windows**: SKIPPED" >> "${GITHUB_STEP_SUMMARY}"
          else
            echo "- âš ï¸ **Windows**: ${windows}" >> "${GITHUB_STEP_SUMMARY}"
          fi

          # macOS Tests
          if [ "${macos}" == "success" ]; then
            echo "- âœ… **macOS**: PASSED (Python 3.12, 3.13)" >> "${GITHUB_STEP_SUMMARY}"
          elif [ "${macos}" == "failure" ]; then
            echo "- âš ï¸ **macOS**: FAILED (Non-blocking)" >> "${GITHUB_STEP_SUMMARY}"
          elif [ "${macos}" == "skipped" ]; then
            echo "- â­ï¸ **macOS**: SKIPPED" >> "${GITHUB_STEP_SUMMARY}"
          else
            echo "- âš ï¸ **macOS**: ${macos}" >> "${GITHUB_STEP_SUMMARY}"
          fi

          {
            echo ""
            echo "## Additional Test Suites"
            echo ""
          } >> "${GITHUB_STEP_SUMMARY}"

          benchmark="${{ needs.weekly-benchmarks.result }}"
          coverage="${{ needs.weekly-coverage-report.result }}"

          # Benchmarks
          if [ "${benchmark}" == "success" ]; then
            echo "- âœ… **Performance Benchmarks**: COMPLETED" >> "${GITHUB_STEP_SUMMARY}"
            # Include benchmark summary if available
            if [ -f "./benchmarks/benchmark-summary.md" ]; then
              {
                echo ""
                echo "### Benchmark Results"
                cat ./benchmarks/benchmark-summary.md
              } >> "${GITHUB_STEP_SUMMARY}"
            fi
          elif [ "${benchmark}" == "failure" ]; then
            echo "- âš ï¸ **Performance Benchmarks**: FAILED (Non-blocking)" >> "${GITHUB_STEP_SUMMARY}"
          elif [ "${benchmark}" == "skipped" ]; then
            echo "- â­ï¸ **Performance Benchmarks**: SKIPPED" >> "${GITHUB_STEP_SUMMARY}"
          else
            echo "- âš ï¸ **Performance Benchmarks**: ${benchmark}" >> "${GITHUB_STEP_SUMMARY}"
          fi

          # Coverage
          if [ "${coverage}" == "success" ]; then
            echo "- âœ… **Coverage Report**: GENERATED" >> "${GITHUB_STEP_SUMMARY}"
          elif [ "${coverage}" == "failure" ]; then
            echo "- âš ï¸ **Coverage Report**: FAILED" >> "${GITHUB_STEP_SUMMARY}"
          else
            echo "- âš ï¸ **Coverage Report**: ${coverage}" >> "${GITHUB_STEP_SUMMARY}"
          fi

          {
            echo ""
            echo "## Overall Status"
            echo ""
          } >> "${GITHUB_STEP_SUMMARY}"

          status="${{ steps.check-results.outputs.status }}"
          if [ "${status}" == "success" ]; then
            echo "âœ… **All critical tests passed across all platforms**" >> "${GITHUB_STEP_SUMMARY}"
          elif [ "${status}" == "warning" ]; then
            echo "âš ï¸ **Linux tests passed, but some platform-specific failures detected**" >> "${GITHUB_STEP_SUMMARY}"
          elif [ "${status}" == "failure" ]; then
            echo "âŒ **Critical test failures detected on Linux**" >> "${GITHUB_STEP_SUMMARY}"
          else
            echo "â“ **Unknown status**" >> "${GITHUB_STEP_SUMMARY}"
          fi

          {
            echo ""
            echo "## Artifacts"
            echo ""
            echo "- Test Results: Check workflow artifacts for platform-specific results"
            echo "- Coverage Report: Full HTML coverage report available in artifacts"
            echo "- Benchmark Results: Performance metrics available in artifacts"
            echo ""
            echo "View full results: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
          } >> "${GITHUB_STEP_SUMMARY}"

      - name: Create issue on failure
        if: steps.check-results.outputs.status == 'failure'
        uses: actions/github-script@v7
        with:
          script: |
            const title = `ðŸ“Š Weekly Tests Failed - Week of ${new Date().toISOString().split('T')[0]}`;

            // Read benchmark summary if available
            const fs = require('fs');
            let benchmarkInfo = '';
            try {
              if (fs.existsSync('./benchmarks/benchmark-summary.md')) {
                benchmarkInfo = '\n\n### Performance Benchmarks\n\n' + 
                  fs.readFileSync('./benchmarks/benchmark-summary.md', 'utf8');
              }
            } catch (err) {
              console.log('No benchmark summary available');
            }

            const body = `## Weekly Comprehensive Test Failure Report

            **Week Ending**: ${new Date().toISOString().split('T')[0]}
            **Test Date**: ${new Date().toUTCString()}
            **Commit**: ${{ github.sha }}
            **Workflow Run**: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}

            ### Test Results

            - Linux Tests: ${{ needs.weekly-comprehensive-linux.result }}
            - Windows Tests: ${{ needs.weekly-comprehensive-windows.result }}
            - macOS Tests: ${{ needs.weekly-comprehensive-macos.result }}
            - Benchmarks: ${{ needs.weekly-benchmarks.result }}
            - Coverage Report: ${{ needs.weekly-coverage-report.result }}
            ${benchmarkInfo}

            ### Action Required

            Critical failures detected on the Linux platform. Please investigate and fix the failing tests.

            Check the workflow run for detailed logs and artifacts including:
            - Test results for each platform
            - Full coverage report
            - Performance benchmark results

            ---
            *This issue was automatically created by the weekly test workflow.*
            `;

            // Check if an open issue already exists
            const issues = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              state: 'open',
              labels: 'weekly-tests,automated',
              per_page: 10
            });

            const existingIssue = issues.data.find(issue =>
              issue.title.includes('Weekly Tests Failed')
            );

            if (existingIssue) {
              // Update existing issue with comment
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: existingIssue.number,
                body: `**New Failure Detected**\n\n${body}`
              });
              console.log(`Updated existing issue #${existingIssue.number}`);
            } else {
              // Create new issue
              const newIssue = await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: title,
                body: body,
                labels: ['weekly-tests', 'automated', 'bug']
              });
              console.log(`Created new issue #${newIssue.data.number}`);
            }

      - name: Fail workflow if critical tests failed
        if: steps.check-results.outputs.status == 'failure'
        run: |
          echo "Critical tests failed on Linux - failing workflow"
          exit 1
